{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463dae00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:09.725909Z",
     "start_time": "2022-07-31T09:18:09.712003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6c4be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.241816Z",
     "start_time": "2022-07-31T09:18:10.288494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from utils.viz import visualize\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM = true\n",
    "\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dec2e",
   "metadata": {},
   "source": [
    "test/ - A folder containing an example essay from the test set. The actual test set comprises about 3,000 essays in a format similar to the training set essays.\n",
    "The test set essays are distinct from the training set essays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1ce74",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2284d8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.245498Z",
     "start_time": "2022-07-31T09:18:12.243371Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = Path('../data/')\n",
    "data_folder = ROOT_DATA/\"feedback-prize-effectiveness\"\n",
    "kfold_name = \"fold_k_5_seed_42\"\n",
    "TARGET = ['Ineffective','Adequate','Effective']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807dfc34",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd28761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:12.367952Z",
     "start_time": "2022-07-31T09:18:12.246739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144287, 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_folder/\"psl_train.csv\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0427efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"discourse_effectiveness\"]='Ineffective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c1c023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:14.353420Z",
     "start_time": "2022-07-31T09:18:14.338502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_idx</th>\n",
       "      <th>dbl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_fold_0_Effective</th>\n",
       "      <th>dbl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_fold_1_Effective</th>\n",
       "      <th>dbl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_fold_2_Effective</th>\n",
       "      <th>dbl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_fold_3_Effective</th>\n",
       "      <th>dbl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.1</th>\n",
       "      <th>dbl_ac_awp_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_0_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_1_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_2_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_3_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_4_Effective</th>\n",
       "      <th>discourse_idx.2</th>\n",
       "      <th>dbxl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_0_Adequate</th>\n",
       "      <th>dbxl_ac_fold_0_Effective</th>\n",
       "      <th>dbxl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_1_Adequate</th>\n",
       "      <th>dbxl_ac_fold_1_Effective</th>\n",
       "      <th>dbxl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_2_Adequate</th>\n",
       "      <th>dbxl_ac_fold_2_Effective</th>\n",
       "      <th>dbxl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_3_Adequate</th>\n",
       "      <th>dbxl_ac_fold_3_Effective</th>\n",
       "      <th>dbxl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_4_Adequate</th>\n",
       "      <th>dbxl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.3</th>\n",
       "      <th>dbl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbl_cd_fold_0_Adequate</th>\n",
       "      <th>dbl_cd_fold_0_Effective</th>\n",
       "      <th>dbl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbl_cd_fold_1_Adequate</th>\n",
       "      <th>dbl_cd_fold_1_Effective</th>\n",
       "      <th>dbl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbl_cd_fold_2_Adequate</th>\n",
       "      <th>dbl_cd_fold_2_Effective</th>\n",
       "      <th>dbl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbl_cd_fold_3_Adequate</th>\n",
       "      <th>dbl_cd_fold_3_Effective</th>\n",
       "      <th>dbl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbl_cd_fold_4_Adequate</th>\n",
       "      <th>dbl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.4</th>\n",
       "      <th>dbxl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_0_Adequate</th>\n",
       "      <th>dbxl_cd_fold_0_Effective</th>\n",
       "      <th>dbxl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_1_Adequate</th>\n",
       "      <th>dbxl_cd_fold_1_Effective</th>\n",
       "      <th>dbxl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_2_Adequate</th>\n",
       "      <th>dbxl_cd_fold_2_Effective</th>\n",
       "      <th>dbxl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_3_Adequate</th>\n",
       "      <th>dbxl_cd_fold_3_Effective</th>\n",
       "      <th>dbxl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_4_Adequate</th>\n",
       "      <th>dbxl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.5</th>\n",
       "      <th>hf43_dr_fold_0_Ineffective</th>\n",
       "      <th>hf43_dr_fold_0_Adequate</th>\n",
       "      <th>hf43_dr_fold_0_Effective</th>\n",
       "      <th>hf43_dr_fold_1_Ineffective</th>\n",
       "      <th>hf43_dr_fold_1_Adequate</th>\n",
       "      <th>hf43_dr_fold_1_Effective</th>\n",
       "      <th>hf43_dr_fold_2_Ineffective</th>\n",
       "      <th>hf43_dr_fold_2_Adequate</th>\n",
       "      <th>hf43_dr_fold_2_Effective</th>\n",
       "      <th>hf43_dr_fold_3_Ineffective</th>\n",
       "      <th>hf43_dr_fold_3_Adequate</th>\n",
       "      <th>hf43_dr_fold_3_Effective</th>\n",
       "      <th>hf43_dr_fold_4_Ineffective</th>\n",
       "      <th>hf43_dr_fold_4_Adequate</th>\n",
       "      <th>hf43_dr_fold_4_Effective</th>\n",
       "      <th>fold_k_5_seed_42</th>\n",
       "      <th>fold_k_5_seed_2020</th>\n",
       "      <th>fold_k_8_seed_42</th>\n",
       "      <th>fold_k_8_seed_2020</th>\n",
       "      <th>fold_k_10_seed_42</th>\n",
       "      <th>fold_k_10_seed_2020</th>\n",
       "      <th>target</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627660524</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.954007</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.967579</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.941951</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.953399</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.036227</td>\n",
       "      <td>0.085879</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.030948</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.907612</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.927665</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.954974</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.952640</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.946509</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.955661</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.907290</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>0.911325</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>0.954009</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.055443</td>\n",
       "      <td>0.936815</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.949759</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.967084</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.945362</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.945428</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>0.939529</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.962005</td>\n",
       "      <td>0.018357</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.963408</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038277</td>\n",
       "      <td>0.944679</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627653021</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.798637</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.124825</td>\n",
       "      <td>0.872499</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.220555</td>\n",
       "      <td>0.776049</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.795569</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.108020</td>\n",
       "      <td>0.889190</td>\n",
       "      <td>0.00279</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.881405</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.145438</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>0.820701</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.056605</td>\n",
       "      <td>0.939123</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>0.918072</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.899708</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>0.956055</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.894190</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.040795</td>\n",
       "      <td>0.954139</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.079454</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.058666</td>\n",
       "      <td>0.927609</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.923556</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.849236</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.113238</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.124125</td>\n",
       "      <td>0.865935</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.128466</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109641</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.103145</td>\n",
       "      <td>0.892557</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.034002</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.091991</td>\n",
       "      <td>0.904461</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.890243</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id   discourse_id discourse_idx  dbl_ac_fold_0_Ineffective  dbl_ac_fold_0_Adequate  dbl_ac_fold_0_Effective  dbl_ac_fold_1_Ineffective  dbl_ac_fold_1_Adequate  dbl_ac_fold_1_Effective  dbl_ac_fold_2_Ineffective  dbl_ac_fold_2_Adequate  dbl_ac_fold_2_Effective  dbl_ac_fold_3_Ineffective  dbl_ac_fold_3_Adequate  dbl_ac_fold_3_Effective  dbl_ac_fold_4_Ineffective  dbl_ac_fold_4_Adequate  dbl_ac_fold_4_Effective discourse_idx.1  dbl_ac_awp_fold_0_Ineffective  \\\n",
       "0  423A1CA112E2  1622627660524  1078c641ef02                   0.034966                0.954007                 0.011027                   0.018977                0.963046                 0.017976                   0.015324                0.967579                 0.017097                   0.042056                0.941951                 0.015993                   0.038436                0.953973                 0.007592    1078c641ef02                       0.035761   \n",
       "1  423A1CA112E2  1622627653021  3695c85c6495                   0.197380                0.798637                 0.003982                   0.076309                0.918173                 0.005518                   0.124825                0.872499                 0.002675                   0.220555                0.776049                 0.003396                   0.202561                0.795569                 0.001870    3695c85c6495                       0.108020   \n",
       "\n",
       "   dbl_ac_awp_fold_0_Adequate  dbl_ac_awp_fold_0_Effective  dbl_ac_awp_fold_1_Ineffective  dbl_ac_awp_fold_1_Adequate  dbl_ac_awp_fold_1_Effective  dbl_ac_awp_fold_2_Ineffective  dbl_ac_awp_fold_2_Adequate  dbl_ac_awp_fold_2_Effective  dbl_ac_awp_fold_3_Ineffective  dbl_ac_awp_fold_3_Adequate  dbl_ac_awp_fold_3_Effective  dbl_ac_awp_fold_4_Ineffective  dbl_ac_awp_fold_4_Adequate  dbl_ac_awp_fold_4_Effective discourse_idx.2  dbxl_ac_fold_0_Ineffective  dbxl_ac_fold_0_Adequate  \\\n",
       "0                    0.953399                      0.01084                       0.067797                    0.895976                     0.036227                       0.085879                    0.883173                     0.030948                       0.078403                    0.907612                     0.013985                       0.062033                    0.927665                     0.010302    1078c641ef02                    0.032995                 0.954974   \n",
       "1                    0.889190                      0.00279                       0.114678                    0.881405                     0.003916                       0.138686                    0.855806                     0.005508                       0.145438                    0.850422                     0.004140                       0.175941                    0.820701                     0.003357    3695c85c6495                    0.056605                 0.939123   \n",
       "\n",
       "   dbxl_ac_fold_0_Effective  dbxl_ac_fold_1_Ineffective  dbxl_ac_fold_1_Adequate  dbxl_ac_fold_1_Effective  dbxl_ac_fold_2_Ineffective  dbxl_ac_fold_2_Adequate  dbxl_ac_fold_2_Effective  dbxl_ac_fold_3_Ineffective  dbxl_ac_fold_3_Adequate  dbxl_ac_fold_3_Effective  dbxl_ac_fold_4_Ineffective  dbxl_ac_fold_4_Adequate  dbxl_ac_fold_4_Effective discourse_idx.3  dbl_cd_fold_0_Ineffective  dbl_cd_fold_0_Adequate  dbl_cd_fold_0_Effective  dbl_cd_fold_1_Ineffective  dbl_cd_fold_1_Adequate  \\\n",
       "0                  0.012031                    0.032231                 0.952640                  0.015129                    0.042135                 0.946509                  0.011356                    0.015863                 0.955661                  0.028476                    0.019859                 0.975816                  0.004325    1078c641ef02                   0.017759                0.961099                 0.021142                   0.078091                0.907290   \n",
       "1                  0.004272                    0.077579                 0.918072                  0.004350                    0.098522                 0.899708                  0.001770                    0.038785                 0.956055                  0.005160                    0.104478                 0.894190                  0.001332    3695c85c6495                   0.040795                0.954139                 0.005066                   0.079454                0.916885   \n",
       "\n",
       "   dbl_cd_fold_1_Effective  dbl_cd_fold_2_Ineffective  dbl_cd_fold_2_Adequate  dbl_cd_fold_2_Effective  dbl_cd_fold_3_Ineffective  dbl_cd_fold_3_Adequate  dbl_cd_fold_3_Effective  dbl_cd_fold_4_Ineffective  dbl_cd_fold_4_Adequate  dbl_cd_fold_4_Effective discourse_idx.4  dbxl_cd_fold_0_Ineffective  dbxl_cd_fold_0_Adequate  dbxl_cd_fold_0_Effective  dbxl_cd_fold_1_Ineffective  dbxl_cd_fold_1_Adequate  dbxl_cd_fold_1_Effective  dbxl_cd_fold_2_Ineffective  dbxl_cd_fold_2_Adequate  \\\n",
       "0                 0.014619                   0.037290                0.911325                 0.051385                   0.038027                0.955099                 0.006874                   0.037068                0.954009                 0.008923    1078c641ef02                    0.024958                 0.960981                  0.014062                    0.055443                 0.936815                  0.007742                    0.048652                 0.947200   \n",
       "1                 0.003661                   0.058666                0.927609                 0.013724                   0.072862                0.923556                 0.003582                   0.147645                0.849236                 0.003119    3695c85c6495                    0.113238                 0.879634                  0.007129                    0.124125                 0.865935                  0.009940                    0.046513                 0.949099   \n",
       "\n",
       "   dbxl_cd_fold_2_Effective  dbxl_cd_fold_3_Ineffective  dbxl_cd_fold_3_Adequate  dbxl_cd_fold_3_Effective  dbxl_cd_fold_4_Ineffective  dbxl_cd_fold_4_Adequate  dbxl_cd_fold_4_Effective  discourse_idx.5  hf43_dr_fold_0_Ineffective  hf43_dr_fold_0_Adequate  hf43_dr_fold_0_Effective  hf43_dr_fold_1_Ineffective  hf43_dr_fold_1_Adequate  hf43_dr_fold_1_Effective  hf43_dr_fold_2_Ineffective  hf43_dr_fold_2_Adequate  hf43_dr_fold_2_Effective  hf43_dr_fold_3_Ineffective  hf43_dr_fold_3_Adequate  \\\n",
       "0                  0.004149                    0.026633                 0.949759                  0.023609                    0.023523                 0.967084                  0.009393              NaN                    0.026505                 0.945362                  0.028132                    0.035739                 0.945428                  0.018833                    0.031681                 0.939529                  0.028790                    0.019637                 0.962005   \n",
       "1                  0.004388                    0.036960                 0.951375                  0.011665                    0.128466                 0.862962                  0.008572              NaN                    0.109641                 0.885871                  0.004488                    0.103145                 0.892557                  0.004298                    0.078563                 0.918519                  0.002918                    0.034002                 0.959838   \n",
       "\n",
       "   hf43_dr_fold_3_Effective  hf43_dr_fold_4_Ineffective  hf43_dr_fold_4_Adequate  hf43_dr_fold_4_Effective  fold_k_5_seed_42  fold_k_5_seed_2020  fold_k_8_seed_42  fold_k_8_seed_2020  fold_k_10_seed_42  fold_k_10_seed_2020  target  Ineffective  Adequate  Effective                                     discourse_text discourse_type discourse_effectiveness  \n",
       "0                  0.018357                    0.024594                 0.963408                  0.011998               4.0                 2.0               5.0                 4.0                8.0                  8.0       1     0.038277  0.944679   0.017044  Modern humans today are always on their phone....           Lead             Ineffective  \n",
       "1                  0.006159                    0.091991                 0.904461                  0.003548               4.0                 2.0               5.0                 4.0                8.0                  8.0       1     0.104881  0.890243   0.004876  They are some really bad consequences when stu...       Position             Ineffective  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c5e783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20972\n",
       "2     9326\n",
       "0     6461\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08239e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dbl_ac_fold_0_Ineffective', 'dbl_ac_awp_fold_0_Ineffective', 'dbxl_ac_fold_0_Ineffective', 'dbl_cd_fold_0_Ineffective', 'dbxl_cd_fold_0_Ineffective', 'hf43_dr_fold_0_Ineffective']\n",
      "['dbl_ac_fold_0_Adequate', 'dbl_ac_awp_fold_0_Adequate', 'dbxl_ac_fold_0_Adequate', 'dbl_cd_fold_0_Adequate', 'dbxl_cd_fold_0_Adequate', 'hf43_dr_fold_0_Adequate']\n",
      "['dbl_ac_fold_0_Effective', 'dbl_ac_awp_fold_0_Effective', 'dbxl_ac_fold_0_Effective', 'dbl_cd_fold_0_Effective', 'dbxl_cd_fold_0_Effective', 'hf43_dr_fold_0_Effective']\n"
     ]
    }
   ],
   "source": [
    "for c in TARGET:\n",
    "    cols = [x for x in train_df.columns if x!=c  if c in x if 'fold_0' in x]\n",
    "    print(cols)\n",
    "    train_df[c] = train_df[cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f14268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fold0 = train_df[train_df[kfold_name]==0]\n",
    "df_fold0[kfold_name] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10,TARGET] = pd.get_dummies(train_df.loc[train_df[kfold_name]!=10].target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af93afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_idx</th>\n",
       "      <th>dbl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_fold_0_Effective</th>\n",
       "      <th>dbl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_fold_1_Effective</th>\n",
       "      <th>dbl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_fold_2_Effective</th>\n",
       "      <th>dbl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_fold_3_Effective</th>\n",
       "      <th>dbl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.1</th>\n",
       "      <th>dbl_ac_awp_fold_0_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_0_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_0_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_1_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_1_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_2_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_2_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_3_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_3_Effective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Ineffective</th>\n",
       "      <th>dbl_ac_awp_fold_4_Adequate</th>\n",
       "      <th>dbl_ac_awp_fold_4_Effective</th>\n",
       "      <th>discourse_idx.2</th>\n",
       "      <th>dbxl_ac_fold_0_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_0_Adequate</th>\n",
       "      <th>dbxl_ac_fold_0_Effective</th>\n",
       "      <th>dbxl_ac_fold_1_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_1_Adequate</th>\n",
       "      <th>dbxl_ac_fold_1_Effective</th>\n",
       "      <th>dbxl_ac_fold_2_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_2_Adequate</th>\n",
       "      <th>dbxl_ac_fold_2_Effective</th>\n",
       "      <th>dbxl_ac_fold_3_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_3_Adequate</th>\n",
       "      <th>dbxl_ac_fold_3_Effective</th>\n",
       "      <th>dbxl_ac_fold_4_Ineffective</th>\n",
       "      <th>dbxl_ac_fold_4_Adequate</th>\n",
       "      <th>dbxl_ac_fold_4_Effective</th>\n",
       "      <th>discourse_idx.3</th>\n",
       "      <th>dbl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbl_cd_fold_0_Adequate</th>\n",
       "      <th>dbl_cd_fold_0_Effective</th>\n",
       "      <th>dbl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbl_cd_fold_1_Adequate</th>\n",
       "      <th>dbl_cd_fold_1_Effective</th>\n",
       "      <th>dbl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbl_cd_fold_2_Adequate</th>\n",
       "      <th>dbl_cd_fold_2_Effective</th>\n",
       "      <th>dbl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbl_cd_fold_3_Adequate</th>\n",
       "      <th>dbl_cd_fold_3_Effective</th>\n",
       "      <th>dbl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbl_cd_fold_4_Adequate</th>\n",
       "      <th>dbl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.4</th>\n",
       "      <th>dbxl_cd_fold_0_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_0_Adequate</th>\n",
       "      <th>dbxl_cd_fold_0_Effective</th>\n",
       "      <th>dbxl_cd_fold_1_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_1_Adequate</th>\n",
       "      <th>dbxl_cd_fold_1_Effective</th>\n",
       "      <th>dbxl_cd_fold_2_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_2_Adequate</th>\n",
       "      <th>dbxl_cd_fold_2_Effective</th>\n",
       "      <th>dbxl_cd_fold_3_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_3_Adequate</th>\n",
       "      <th>dbxl_cd_fold_3_Effective</th>\n",
       "      <th>dbxl_cd_fold_4_Ineffective</th>\n",
       "      <th>dbxl_cd_fold_4_Adequate</th>\n",
       "      <th>dbxl_cd_fold_4_Effective</th>\n",
       "      <th>discourse_idx.5</th>\n",
       "      <th>hf43_dr_fold_0_Ineffective</th>\n",
       "      <th>hf43_dr_fold_0_Adequate</th>\n",
       "      <th>hf43_dr_fold_0_Effective</th>\n",
       "      <th>hf43_dr_fold_1_Ineffective</th>\n",
       "      <th>hf43_dr_fold_1_Adequate</th>\n",
       "      <th>hf43_dr_fold_1_Effective</th>\n",
       "      <th>hf43_dr_fold_2_Ineffective</th>\n",
       "      <th>hf43_dr_fold_2_Adequate</th>\n",
       "      <th>hf43_dr_fold_2_Effective</th>\n",
       "      <th>hf43_dr_fold_3_Ineffective</th>\n",
       "      <th>hf43_dr_fold_3_Adequate</th>\n",
       "      <th>hf43_dr_fold_3_Effective</th>\n",
       "      <th>hf43_dr_fold_4_Ineffective</th>\n",
       "      <th>hf43_dr_fold_4_Adequate</th>\n",
       "      <th>hf43_dr_fold_4_Effective</th>\n",
       "      <th>fold_k_5_seed_42</th>\n",
       "      <th>fold_k_5_seed_2020</th>\n",
       "      <th>fold_k_8_seed_42</th>\n",
       "      <th>fold_k_8_seed_2020</th>\n",
       "      <th>fold_k_10_seed_42</th>\n",
       "      <th>fold_k_10_seed_2020</th>\n",
       "      <th>target</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627660524</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.954007</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.967579</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.941951</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.953399</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.036227</td>\n",
       "      <td>0.085879</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.030948</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.907612</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.927665</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.954974</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.952640</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.042135</td>\n",
       "      <td>0.946509</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.955661</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.975816</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.907290</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>0.911325</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>0.954009</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>1078c641ef02</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.960981</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.055443</td>\n",
       "      <td>0.936815</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.949759</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.967084</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.945362</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.945428</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>0.939529</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.962005</td>\n",
       "      <td>0.018357</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.963408</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627653021</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.798637</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.124825</td>\n",
       "      <td>0.872499</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.220555</td>\n",
       "      <td>0.776049</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.795569</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.108020</td>\n",
       "      <td>0.889190</td>\n",
       "      <td>0.00279</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.881405</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.145438</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>0.820701</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.056605</td>\n",
       "      <td>0.939123</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>0.918072</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.899708</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>0.956055</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.894190</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.040795</td>\n",
       "      <td>0.954139</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.079454</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.058666</td>\n",
       "      <td>0.927609</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.923556</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.147645</td>\n",
       "      <td>0.849236</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>3695c85c6495</td>\n",
       "      <td>0.113238</td>\n",
       "      <td>0.879634</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.124125</td>\n",
       "      <td>0.865935</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.951375</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.128466</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109641</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.103145</td>\n",
       "      <td>0.892557</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.034002</td>\n",
       "      <td>0.959838</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.091991</td>\n",
       "      <td>0.904461</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Ineffective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id   discourse_id discourse_idx  dbl_ac_fold_0_Ineffective  dbl_ac_fold_0_Adequate  dbl_ac_fold_0_Effective  dbl_ac_fold_1_Ineffective  dbl_ac_fold_1_Adequate  dbl_ac_fold_1_Effective  dbl_ac_fold_2_Ineffective  dbl_ac_fold_2_Adequate  dbl_ac_fold_2_Effective  dbl_ac_fold_3_Ineffective  dbl_ac_fold_3_Adequate  dbl_ac_fold_3_Effective  dbl_ac_fold_4_Ineffective  dbl_ac_fold_4_Adequate  dbl_ac_fold_4_Effective discourse_idx.1  dbl_ac_awp_fold_0_Ineffective  \\\n",
       "0  423A1CA112E2  1622627660524  1078c641ef02                   0.034966                0.954007                 0.011027                   0.018977                0.963046                 0.017976                   0.015324                0.967579                 0.017097                   0.042056                0.941951                 0.015993                   0.038436                0.953973                 0.007592    1078c641ef02                       0.035761   \n",
       "1  423A1CA112E2  1622627653021  3695c85c6495                   0.197380                0.798637                 0.003982                   0.076309                0.918173                 0.005518                   0.124825                0.872499                 0.002675                   0.220555                0.776049                 0.003396                   0.202561                0.795569                 0.001870    3695c85c6495                       0.108020   \n",
       "\n",
       "   dbl_ac_awp_fold_0_Adequate  dbl_ac_awp_fold_0_Effective  dbl_ac_awp_fold_1_Ineffective  dbl_ac_awp_fold_1_Adequate  dbl_ac_awp_fold_1_Effective  dbl_ac_awp_fold_2_Ineffective  dbl_ac_awp_fold_2_Adequate  dbl_ac_awp_fold_2_Effective  dbl_ac_awp_fold_3_Ineffective  dbl_ac_awp_fold_3_Adequate  dbl_ac_awp_fold_3_Effective  dbl_ac_awp_fold_4_Ineffective  dbl_ac_awp_fold_4_Adequate  dbl_ac_awp_fold_4_Effective discourse_idx.2  dbxl_ac_fold_0_Ineffective  dbxl_ac_fold_0_Adequate  \\\n",
       "0                    0.953399                      0.01084                       0.067797                    0.895976                     0.036227                       0.085879                    0.883173                     0.030948                       0.078403                    0.907612                     0.013985                       0.062033                    0.927665                     0.010302    1078c641ef02                    0.032995                 0.954974   \n",
       "1                    0.889190                      0.00279                       0.114678                    0.881405                     0.003916                       0.138686                    0.855806                     0.005508                       0.145438                    0.850422                     0.004140                       0.175941                    0.820701                     0.003357    3695c85c6495                    0.056605                 0.939123   \n",
       "\n",
       "   dbxl_ac_fold_0_Effective  dbxl_ac_fold_1_Ineffective  dbxl_ac_fold_1_Adequate  dbxl_ac_fold_1_Effective  dbxl_ac_fold_2_Ineffective  dbxl_ac_fold_2_Adequate  dbxl_ac_fold_2_Effective  dbxl_ac_fold_3_Ineffective  dbxl_ac_fold_3_Adequate  dbxl_ac_fold_3_Effective  dbxl_ac_fold_4_Ineffective  dbxl_ac_fold_4_Adequate  dbxl_ac_fold_4_Effective discourse_idx.3  dbl_cd_fold_0_Ineffective  dbl_cd_fold_0_Adequate  dbl_cd_fold_0_Effective  dbl_cd_fold_1_Ineffective  dbl_cd_fold_1_Adequate  \\\n",
       "0                  0.012031                    0.032231                 0.952640                  0.015129                    0.042135                 0.946509                  0.011356                    0.015863                 0.955661                  0.028476                    0.019859                 0.975816                  0.004325    1078c641ef02                   0.017759                0.961099                 0.021142                   0.078091                0.907290   \n",
       "1                  0.004272                    0.077579                 0.918072                  0.004350                    0.098522                 0.899708                  0.001770                    0.038785                 0.956055                  0.005160                    0.104478                 0.894190                  0.001332    3695c85c6495                   0.040795                0.954139                 0.005066                   0.079454                0.916885   \n",
       "\n",
       "   dbl_cd_fold_1_Effective  dbl_cd_fold_2_Ineffective  dbl_cd_fold_2_Adequate  dbl_cd_fold_2_Effective  dbl_cd_fold_3_Ineffective  dbl_cd_fold_3_Adequate  dbl_cd_fold_3_Effective  dbl_cd_fold_4_Ineffective  dbl_cd_fold_4_Adequate  dbl_cd_fold_4_Effective discourse_idx.4  dbxl_cd_fold_0_Ineffective  dbxl_cd_fold_0_Adequate  dbxl_cd_fold_0_Effective  dbxl_cd_fold_1_Ineffective  dbxl_cd_fold_1_Adequate  dbxl_cd_fold_1_Effective  dbxl_cd_fold_2_Ineffective  dbxl_cd_fold_2_Adequate  \\\n",
       "0                 0.014619                   0.037290                0.911325                 0.051385                   0.038027                0.955099                 0.006874                   0.037068                0.954009                 0.008923    1078c641ef02                    0.024958                 0.960981                  0.014062                    0.055443                 0.936815                  0.007742                    0.048652                 0.947200   \n",
       "1                 0.003661                   0.058666                0.927609                 0.013724                   0.072862                0.923556                 0.003582                   0.147645                0.849236                 0.003119    3695c85c6495                    0.113238                 0.879634                  0.007129                    0.124125                 0.865935                  0.009940                    0.046513                 0.949099   \n",
       "\n",
       "   dbxl_cd_fold_2_Effective  dbxl_cd_fold_3_Ineffective  dbxl_cd_fold_3_Adequate  dbxl_cd_fold_3_Effective  dbxl_cd_fold_4_Ineffective  dbxl_cd_fold_4_Adequate  dbxl_cd_fold_4_Effective  discourse_idx.5  hf43_dr_fold_0_Ineffective  hf43_dr_fold_0_Adequate  hf43_dr_fold_0_Effective  hf43_dr_fold_1_Ineffective  hf43_dr_fold_1_Adequate  hf43_dr_fold_1_Effective  hf43_dr_fold_2_Ineffective  hf43_dr_fold_2_Adequate  hf43_dr_fold_2_Effective  hf43_dr_fold_3_Ineffective  hf43_dr_fold_3_Adequate  \\\n",
       "0                  0.004149                    0.026633                 0.949759                  0.023609                    0.023523                 0.967084                  0.009393              NaN                    0.026505                 0.945362                  0.028132                    0.035739                 0.945428                  0.018833                    0.031681                 0.939529                  0.028790                    0.019637                 0.962005   \n",
       "1                  0.004388                    0.036960                 0.951375                  0.011665                    0.128466                 0.862962                  0.008572              NaN                    0.109641                 0.885871                  0.004488                    0.103145                 0.892557                  0.004298                    0.078563                 0.918519                  0.002918                    0.034002                 0.959838   \n",
       "\n",
       "   hf43_dr_fold_3_Effective  hf43_dr_fold_4_Ineffective  hf43_dr_fold_4_Adequate  hf43_dr_fold_4_Effective  fold_k_5_seed_42  fold_k_5_seed_2020  fold_k_8_seed_42  fold_k_8_seed_2020  fold_k_10_seed_42  fold_k_10_seed_2020  target  Ineffective  Adequate  Effective                                     discourse_text discourse_type discourse_effectiveness  \n",
       "0                  0.018357                    0.024594                 0.963408                  0.011998               4.0                 2.0               5.0                 4.0                8.0                  8.0       1          0.0       1.0        0.0  Modern humans today are always on their phone....           Lead             Ineffective  \n",
       "1                  0.006159                    0.091991                 0.904461                  0.003548               4.0                 2.0               5.0                 4.0                8.0                  8.0       1          0.0       1.0        0.0  They are some really bad consequences when stu...       Position             Ineffective  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9252430e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ineffective     6461.0\n",
       "Adequate       20972.0\n",
       "Effective       9326.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df[kfold_name]!=10,TARGET].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7845573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    7381\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fold0[kfold_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55bb8f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    107528\n",
       "4.0       7441\n",
       "0.0       7381\n",
       "1.0       7343\n",
       "2.0       7318\n",
       "3.0       7276\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[kfold_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34a042c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151668, 111)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df,df_fold0],axis=0).reset_index(drop=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a888b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144287, 111)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10effb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:16.773094Z",
     "start_time": "2022-07-31T09:18:16.686826Z"
    }
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed = 2022\n",
    "    \n",
    "    # Model\n",
    "    model_name = \"microsoft/deberta-large\" #\"funnel-transformer/large\" #\"allenai/longformer-large-4096\"\n",
    "    \n",
    "    # CV\n",
    "    kfold_name = \"fold_k_5_seed_42\"\n",
    "    selected_folds = [0]\n",
    "    \n",
    "    # Paths\n",
    "    data_folder = ROOT_DATA/\"feedback-prize-2021\"\n",
    "    name = \"deberta_large\"\n",
    "    checkpoints_path = Path(fr'../checkpoint/{kfold_name}/{name}/psl')\n",
    "    pretrained_path = Path(fr'../checkpoint/MLM/{name}/')\n",
    "    add_special_tokens = True\n",
    "    input_type = \"cls_end\"\n",
    "    sample = False\n",
    "    mask_pct = 0\n",
    "    use_dropout = False\n",
    "    use_gradient_checkpointing = False\n",
    "    use_awp = False\n",
    "    val_score_min = 1.\n",
    "    grad_clip = False\n",
    "    add_soft_labels_fold = True\n",
    "    # Names\n",
    "    checkpoints_name = 'log'   \n",
    "    \n",
    "    model = {\n",
    "            \"max_len\":2048,\n",
    "            \"max_len_eval\":2048,\n",
    "            'loss':\"nn.CrossEntropyLoss\",\n",
    "            'pretrained_config':Path(fr'../checkpoint/MLM/{name}/config.pth'),\n",
    "            \"pretrained_weights\":None,\n",
    "            \"pretrained_tokenizer\":Path(fr'../checkpoint/MLM/{name}/tokenizer'),\n",
    "            \"num_labels\":3,\n",
    "            \"model_name\":model_name\n",
    "            }\n",
    "    \n",
    "    optimizer = {\n",
    "            \"name\":\"optim.AdamW\",\n",
    "            'params':{\"lr\":4e-6,'eps':1e-6,\"betas\":[0.9, 0.999],\n",
    "                     \"weight_decay\": 0.01\n",
    "                     },            \n",
    "            }\n",
    "\n",
    "    scheduler = {\n",
    "            \"name\":\"poly\",\n",
    "            'params':{\n",
    "                      \"lr_end\":3e-7,\"power\":3,\"epochs\":4\n",
    "                     },\n",
    "            \"warmup\":0.1,            \n",
    "            }\n",
    "    \n",
    "    train_loader = {\n",
    "            \"batch_size\":2,\n",
    "            'drop_last':True,\n",
    "            \"num_workers\":15,\n",
    "            \"pin_memory\":False,\n",
    "            \"shuffle\":True,\n",
    "            }\n",
    "    \n",
    "    val_loader = {\n",
    "            \"batch_size\":1,\n",
    "            'drop_last':False,\n",
    "            \"num_workers\":15,\n",
    "            \"pin_memory\":False,\n",
    "            \"shuffle\":False\n",
    "            }\n",
    "    trainer = {\"use_amp\":True,'epochs':4}\n",
    "    callbacks = {'save':True,\"es\":True,\"patience\":3,\n",
    "                 'verbose_eval':1,\"epoch_pct_eval\":1/10,\"epoch_eval_dist\":\"uniforme\",\n",
    "                 \"metric_track\":\"val_loss\",\"mode\":\"min\",'top_k':3,\"softmax_before\":0\n",
    "                }\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    \n",
    "args.checkpoints_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b44bff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T09:18:17.787929Z",
     "start_time": "2022-07-31T09:18:17.582814Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_utils_awp_psl import kfold,FeedbackModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bddd7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T10:09:53.140449Z",
     "start_time": "2022-07-31T09:18:18.377058Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50279\n",
      "----------- fold_k_5_seed_42 ---------\n",
      "\n",
      "-------------   Fold 1 / 6  -------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c74edb23ac74c9eb8f179f715419409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609d2c39d1d4ee79e039f9b9fc9d49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d5be2bae8742aa8d6a23b5870adf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f60bd2a9f6f4d159c716f05815686e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pretrained Weights\n",
      "../checkpoint/MLM/deberta_large/_epoch=7 _step=1 _fold=0 _val_loss=1.3127 _val_log_loss=1.3127 _train_loss=1.7996.pth\n",
      "reseizing in nn\n",
      "Using Pretrained Weights\n",
      "../checkpoint/MLM/deberta_large/_epoch=7 _step=1 _fold=0 _val_loss=1.3127 _val_log_loss=1.3127 _train_loss=1.7996.pth\n",
      "    -> 405180419 trainable parameters\n",
      "\n",
      "Using Amp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5cabd41308420bae3a6a285d8a7ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [cls_lead] You and your friends and family could often have arguments on whether something is real or not , such as in movies . Some might think one thing is special effects , others will think it is real . In Un mask ing the Face on Mars , the author explains controversy of the \" face \" found on Mars ' surface . It became an icon once it was released to the public . It was highlighted for decades and the society argued over it being an alien making , or just a land form called a mes a . [end_lead]  [cls_position] The face was just a natural land form and it shouldn 't have been thought as an alien form . [end_position]  [cls_claim] The most logical answer to this was that it was a mes a , and that the shadows on the surface happened to make it look like a human face of an e gypt ian ph araoh . [end_claim]  [cls_evidence] This struck civilization in the late 70 s and caused a lot of arguments . The people that thought is actually was an alien structure were just people of the press who just wanted attention . A Mars Global Survey or \" snapped a picture ten times sharper than the original Viking photos . Thousands of anxious web surf ers were waiting when the image first appeared an a J PL web site , revealing ... a natural land form . There was no alien monument after all .\" This quotes directly states and gives evidence that it is just a natural land form . [end_evidence]  [cls_claim] There isn 't even any true or logical evidence that the face on Mars was an \" alien artifact .\" It was just a rumor that went around to keep things interesting and intrigue big movie companies . [end_claim]  [cls_evidence] The cameras even took a picture with a ten times better resolution than the 1976 picture of the face . In addition , the new picture even had each pixel in the image span 1 . 56 meters in comparison to the original 43 meters per pixel . Gar vin talks that \" As a rule of thumb you can discern things in a digital image 3 times bigger than the pixel size ,\" and that \" if there were objects in this picture like airplanes on the ground or Egyptian - style py ramids or even small sh acks , you could see what they were !\" Gar vin directly states here that no matter what , the formation couldn 't have possibly been an alien formation or even a pyramid . This is even more proof that it is ridiculous to think that the formation was an \" alien making .\" [end_evidence]  [cls_counterclaim] Although the face was thought to be alien [end_counterclaim] ,  [cls_rebuttal] there is too much evidence that denies it , and that it isn 't . [end_rebuttal]  [cls_evidence] Scientists would wish that it was an extra terrestrial sign , but the odds of that are too scarce to even consider . This was a very controversial topic , as I have stated before . All of the theorists out there had many different and unique opinions of it . The narrator says what the image \" actually shows is the Martian equivalent of a but te or mes a - land from s common around the American West . \" It rem e ind s me most of Middle But te in the Snake River Plain of Idaho ,\" says Gar vin .\" This evidence gives even further support that it isn 't extra terrestrial , and states that it isn 't , sh ich exposes that it is a fl uke . [end_evidence]  [cls_concluding] The face on Mars was an ongoing argument for years to come once it was released to the public . There was no reason to do so . There was already valid points that gave proof of it being just a mes a . Things like this occ ure nce happen all the time , even in your daily life . If things like this run across your mind , think about it the logical way and not a very , very unlikely path about it . There may be similarities between the sides , like in this argument . But , there was too much to show that it favored the more realistic side of it . The passage has multiple occ ure nces where it directly states just that . [end_concluding] [SEP]\n",
      "Epoch 1.1/10 lr=0.000004 t=340s   train_loss=0.0823   val_loss=0.6807  val_log_loss=0.6807 \n",
      "Epoch 1.2/10 lr=0.000004 t=654s   train_loss=0.1498   val_loss=0.6226  val_log_loss=0.6226 \n",
      "Epoch 1.3/10 lr=0.000004 t=967s   train_loss=0.2147   val_loss=0.6004  val_log_loss=0.6004 \n",
      "Epoch 1.4/10 lr=0.000004 t=1275s   train_loss=0.2785   val_loss=0.5951  val_log_loss=0.5951 \n",
      "Epoch 1.5/10 lr=0.000004 t=1591s   train_loss=0.3413   val_loss=0.5935  val_log_loss=0.5935 \n",
      "Epoch 1.6/10 lr=0.000003 t=1903s   train_loss=0.4036   val_loss=0.5876  val_log_loss=0.5876 \n",
      "Epoch 1.7/10 lr=0.000003 t=2227s   train_loss=0.4652   val_loss=0.5839  val_log_loss=0.5839 \n",
      "Epoch 1.8/10 lr=0.000003 t=2539s   train_loss=0.5276   val_loss=0.5785  val_log_loss=0.5785 \n",
      "Epoch 1.9/10 lr=0.000003 t=2856s   train_loss=0.5900   val_loss=0.5730  val_log_loss=0.5730 \n",
      "Epoch 1.10/10 lr=0.000003 t=3168s   train_loss=0.6520   val_loss=0.5758  val_log_loss=0.5758 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa40a967545437381220e155f4f169e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 47.51 GiB total capacity; 36.37 GiB already allocated; 85.94 MiB free; 39.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:720\u001b[0m, in \u001b[0;36mkfold\u001b[0;34m(args, df)\u001b[0m\n\u001b[1;32m    717\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m df[df[args\u001b[38;5;241m.\u001b[39mkfold_name]\u001b[38;5;241m!=\u001b[39mi]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.sample(100)\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     valid_df \u001b[38;5;241m=\u001b[39m df[df[args\u001b[38;5;241m.\u001b[39mkfold_name]\u001b[38;5;241m==\u001b[39mi]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#.sample(100)\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:641\u001b[0m, in \u001b[0;36mtrain_one_fold\u001b[0;34m(args, tokenizer, train_df, valid_df, fold)\u001b[0m\n\u001b[1;32m    638\u001b[0m n_parameters \u001b[38;5;241m=\u001b[39m count_parameters(model)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trainable parameters\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 641\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mfit_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_val\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:501\u001b[0m, in \u001b[0;36mfit_net\u001b[0;34m(model, train_dataset, val_dataset, args, fold, tokenizer)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m==\u001b[39mepoch \u001b[38;5;129;01mand\u001b[39;00m step\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(train_dataset\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m--> 501\u001b[0m loss,tr_sc\u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(tr_sc)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tr_sc\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/train_utils_awp.py:241\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(args, model, criterion, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtrainer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_amp\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 241\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Kaggle/Script/NLP/FPrize Effective Arguments/src/model_zoo/models.py:84\u001b[0m, in \u001b[0;36mFeedbackModel.forward\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,b):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:954\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    944\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    946\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    947\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    948\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    951\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    952\u001b[0m )\n\u001b[0;32m--> 954\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:447\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    438\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    439\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    440\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m         rel_embeddings,\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    457\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:352\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    351\u001b[0m ):\n\u001b[0;32m--> 352\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    361\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:285\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    284\u001b[0m ):\n\u001b[0;32m--> 285\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    294\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:648\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    645\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_logits_proj(attention_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    647\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m XSoftmax\u001b[38;5;241m.\u001b[39mapply(attention_scores, attention_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 648\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n\u001b[1;32m    650\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_weights_proj(attention_probs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:208\u001b[0m, in \u001b[0;36mStableDropout.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mCall the module\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    x (`torch.tensor`): The input tensor to apply dropout\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXDropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:169\u001b[0m, in \u001b[0;36mXDropout.forward\u001b[0;34m(ctx, input, local_ctx)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m, local_ctx):\n\u001b[0;32m--> 169\u001b[0m     mask, dropout \u001b[38;5;241m=\u001b[39m \u001b[43mget_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:155\u001b[0m, in \u001b[0;36mget_mask\u001b[0;34m(input, local_context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     mask \u001b[38;5;241m=\u001b[39m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mreuse_mask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbernoulli_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dropout))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(local_context, DropoutContext):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_context\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 47.51 GiB total capacity; 36.37 GiB already allocated; 85.94 MiB free; 39.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "kfold(args,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe8963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
